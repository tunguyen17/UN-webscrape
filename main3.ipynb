{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# REGEX compiled expressions\n",
    "tweet_re = re.compile(r'#[a-zA-z]+[0-9]+')\n",
    "\n",
    "goals_re = re.compile(r'Goal [0-9]+')\n",
    "\n",
    "des_separator_re = re.compile(r'<div id=\"subHeadline\">')\n",
    "htmltag_re = re.compile(r'<[^>]*>')\n",
    "newline_re = re.compile(r'[\\n\\r]+')\n",
    "\n",
    "#*****************************************\n",
    "\n",
    "# FUNCTIONS\n",
    "\n",
    "# Title\n",
    "def get_title():\n",
    "    return soup.find(id = 'headline').get_text().strip()\n",
    "\n",
    "# Goals\n",
    "def get_goals():\n",
    "    if len(r.history) and 'the ocean conference' in soup.title.text.lower():\n",
    "\n",
    "\n",
    "        other_sgd_ind = home_right_raw.index('Other SDGs')\n",
    "\n",
    "        other_goals = re.findall(goals_re,home_right_raw[other_sgd_ind:])\n",
    "\n",
    "        goals_lst = ['Goal 14'] + other_goals\n",
    "\n",
    "    else:\n",
    "        goals_raw = soup.find(id='targets')\n",
    "        goals_lst = [goal.get_text() for goal in goals_raw.findAll('strong')]\n",
    "        \n",
    "    return ','.join(goals_lst)\n",
    "\n",
    "# Partners\n",
    "def get_partners():\n",
    "    partner_index = home_right_raw_lst.index('Partners') + 1\n",
    "    next_index = home_right_raw_lst.index('Ocean Basins') \\\n",
    "                    if len(r.history) and 'the ocean conference' in soup.title.text.lower() \\\n",
    "                    else home_right_raw_lst.index('Countries')\n",
    "            \n",
    "    partners = [p.strip() for p in home_right_raw_lst[partner_index:next_index]]\n",
    "    partners = list(filter(None, partners))\n",
    "    \n",
    "    return '; '.join(partners)\n",
    "\n",
    "# Description\n",
    "def get_description():\n",
    "    des_raw = soup.find(id='intro').find('div', attrs={'class':'wrap'})\n",
    "\n",
    "    temp = str(des_raw)\n",
    "    \n",
    "    temp = re.sub(htmltag_re, '', temp)\n",
    "    temp = re.sub(newline_re, '', temp)\n",
    "    \n",
    "    if '<div id=\"subHeadline\">' in temp:\n",
    "        temp = re.sub(des_separator_re, ' : ', temp)    \n",
    "        return temp.strip()\n",
    "    else: \n",
    "        return temp.strip()\n",
    "    \n",
    "def get_resources():\n",
    "    resources_raw = soup.find(id='resources')\n",
    "    resources_lst = []\n",
    "\n",
    "    for resource in resources_raw.findAll('div', recursive = False):\n",
    "        temp = resource.get_text()\n",
    "\n",
    "        if temp != '':\n",
    "            temp = re.sub(r'\\n+', ' : ', temp.strip())\n",
    "#             temp = re.sub(r'[\\x91\\x92]', '\\'', temp)\n",
    "            resources_lst.append(temp.strip())\n",
    "    return '; '.join(resources_lst)\n",
    "\n",
    "\n",
    "def get_timeframe():\n",
    "    time_frame_index  = [i for i, item in enumerate(home_right_raw_lst) if re.search('Time-frame', item)]\n",
    "    return home_right_raw_lst[time_frame_index[0]] if len(time_frame_index) == 1 else 'Time-frame: '\n",
    "\n",
    "def get_countries():\n",
    "    try:\n",
    "        countries_index = home_right_raw_lst.index('Countries') + 1\n",
    "        next_index = home_right_raw_lst.index('Contact information')\n",
    "        countries = home_right_raw_lst[countries_index:next_index]\n",
    "        return \",\".join(countries)\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "def get_hashtag():\n",
    "    try:\n",
    "        return list(filter(tweet_re.match, home_right_raw_lst))[0]\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*****************************************\n",
    "\n",
    "# SETTING UP VARIABLES\n",
    "base_url = 'https://sustainabledevelopment.un.org/partnership/?p='\n",
    "\n",
    "functions = [get_title, get_goals, get_partners, get_description,\\\n",
    "             get_resources, get_timeframe, get_countries, get_hashtag]\n",
    "\n",
    "ids = open('good_ids.txt').read().split()\n",
    "\n",
    "from random import sample\n",
    "sub_ids = sample(ids, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_file = open('data.csv', 'w')\n",
    "# data_file.write('Project_idx\\tTitle\\tGoals\\tPartners\\tDescription\\tResources\\tTime_frame\\tCountries\\tHashtag\\n')\n",
    "\n",
    "df_raw = []\n",
    "\n",
    "for ide in sub_ids:\n",
    "    \n",
    "    project_idx = '0'*(5 - len(ide)) + ide\n",
    "    \n",
    "    url = base_url + ide\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    raw_data = r.text\n",
    "    \n",
    "#     print(' ******************************************  ')\n",
    "#     print(url)\n",
    "    \n",
    "    soup = BeautifulSoup(raw_data, 'html.parser')\n",
    "\n",
    "    home_right = soup.find('div', attrs={'class':'homeRight'})\n",
    "    \n",
    "    home_right_raw = str(home_right)\n",
    "\n",
    "    home_right_raw_lst = home_right.getText().split('\\n')\n",
    "    home_right_raw_lst = list(filter(None, home_right_raw_lst))\n",
    "\n",
    "    row = [f() for f in functions]\n",
    "\n",
    "    row.insert(0, repr(project_idx))\n",
    "    df_raw.append(row)\n",
    "    \n",
    "#     print(row)\n",
    "#     row = '\\t'.join(row)\n",
    "#     row = repr(project_idx) + '\\t' + row + '\\n'\n",
    "#     data_file.write(row)\n",
    "    \n",
    "#     for f in functions:\n",
    "#         print('\\n --------- \\n')\n",
    "#         print(f.__name__)\n",
    "#         print('\\n')\n",
    "#         print(repr(f()))\n",
    "        \n",
    "# data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_raw, columns=['Project_idx', 'Title','Goals', 'Partners', \\\n",
    "                                  'Description', 'Resources', 'Time_frame' ,'Countries', 'Hashtag'] )\n",
    "\n",
    "writer = pd.ExcelWriter('output.xlsx')\n",
    "df.to_excel(writer, index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-1978a6691201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdaata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n"
     ]
    }
   ],
   "source": [
    "daata = pd.read_csv('data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "daata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
